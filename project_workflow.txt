
1. conda create -n network_security python==3.10 
    conda activate network_security

2.  create project structure 
    -   .github/workflow/main.yaml for deployment
    -   Network_data for storing the Network_data
    -   networksecurity
            -cloud
            -components
            -constants
            -entity
            -exception
            -logging
            -pipeline
            -utils
            
    - notebooks
    -   .env
    -   .gitignore
    -   DockerFile
    -   project_workflow.txt
    -   README.md
    -   requirements.txt
    -   setup.py

Create a __init__.py file under all subfolders of networksecurity

3. Create the setup.py file
    - All the packages should be build before the execution of the project

4. do pip install -r requirements.txt
    -   This will install all the packages mentioned in the requirements file
    - This help in packaging the entire project

5. Create the logging and exception Module

6. ETL Pipeline

Source to extract:
APIs
S3 Bucket
Paid APIs

Transform:
Basic Preprocessing
json

Load:
MongoDB --> Atlas cloud
AWS Dynamdb
SQL
S3 Bucket

7. setup MongoDB connections
push_data.py
keep the mongoDB url in .env variable and dont push it to github
pushed from csv to mongoDB --> 11055 records

DATABASE="network_security"
COLLECTION_NAME="network_data"

8. Data ingestion 
lecture - 286 insert diagram in readme (7:58)
create config_entity
add Data Ingestion related constant
create the data_ingestion.py

    - export_collection_as_dataframe (mondoDB to DF)
    - export_data_into_feature_store ( save from DF to feature store so that everytime we dont take from mongoDB)
    - 


9. Data validation
lecture - 287 (notes 4:00)
This data validation is slightly different from vehicle insurance
we are checking the data drift between train and test df and comparing with threshold
check code once, not appropriate

10. Data Transformation
KNN imputer, read data from Data ingestion artifact, in vehicle insurance it was read in that way, could had been read from data validation artifact also.

11. Model Trainer:
-
- add mflow to track experiments
- add dagshub in model_trainer (Collaborative Environment in remote repo) 
        - Dagshub is a remote repor + DVC + experiment tracking
- create training Pipeline
- Create app.py and create a fastAPI

Issue observed:
1. connection issue when trying to connect to dagshub
2. when trying to train via swagger ui failing

12. Push Final Model and Artifact to AWS S3
    -   Push artifact and model to AWS S3, before that create a bucket
    - create an IAM user and do AWS configure in terminal
    - uvicorn app:app --reload
    - once you trigger train  from swagger UI the new model and artifacts will be created in the bucket.

13. Build Docker Image and Github actions

Industry Standard:

< Network_security API > --> < Docker Image > --> < AWS ECR/dockerhub > ---> < EC2(self hosted runner) >
< ------------------- C Improvement -----------------C Delivery ----------------Continuous Deployment  >


14. Github actions, dcoker image push to AWS ECR

-   Create the docker File
-   Configure github actions (main.yaml)
- Create a ECR repo before

Note add these in github secrets:
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
AWS_DEFAULT_REGION
ECR_REPO
AWS_ECR_LOGIN_URI


15. Final deployment to EC2

- Continuous Deployment runs on self hosted i.e EC2

Self Hosted: keeps on listening is we are pushing anything in github. It is like a event listener.

Summary Table:
Feature	            Self-Hosted on EC2	    EKS-Based (Kubernetes)
------------------------------------------------------------------------
Cost	            Low	                    Higher
Scalability	        Limited	                High
Setup complexity	Simple	                Complex
Resilience	        Manual restart needed	Auto-healing via Kubernetes
Best for	        MVPs, small apps	    Production-scale ML systems

- Setup EC2 has aself hosted runner:
    - create a EC2 instance(t2.medium)
    - connect using EC2 instance connect
    - Docker command in EC2 setup to be executed
    - github --> actions --> runners --> new self hosted runner
    - add the code in EC2 and complete the setup of ec2 self hosted runner.
    - final command - ./run.sh 
    - in EC2 instance - security - security group - inbound rule - <custom TCP> <port 5000> --- <0.0.0.0>
     

----------------------------------------------------------
Complete
----------------------------------------------------------
