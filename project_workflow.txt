
1. conda create -n network_security python==3.10 
    conda activate network_security

2.  create project structure 
    -   .github/workflow/main.yaml for deployment
    -   Network_data for storing the Network_data
    -   networksecurity
            -cloud
            -components
            -constants
            -entity
            -exception
            -logging
            -pipeline
            -utils
            
    - notebooks
    -   .env
    -   .gitignore
    -   DockerFile
    -   project_workflow.txt
    -   README.md
    -   requirements.txt
    -   setup.py

Create a __init__.py file under all subfolders of networksecurity

3. Create the setup.py file
    - All the packages should be build before the execution of the project

4. do pip install -r requirements.txt
    -   This will install all the packages mentioned in the requirements file
    - This help in packaging the entire project

5. Create the logging and exception Module

6. ETL Pipeline

Source to extract:
APIs
S3 Bucket
Paid APIs

Transform:
Basic Preprocessing
json

Load:
MongoDB --> Atlas cloud
AWS Dynamdb
SQL
S3 Bucket

7. setup MongoDB connections
push_data.py
keep the mongoDB url in .env variable and dont push it to github
pushed from csv to mongoDB --> 11055 records

DATABASE="network_security"
COLLECTION_NAME="network_data"

8. Data ingestion 
lecture - 286 insert diagram in readme (7:58)
create config_entity
add Data Ingestion related constant
create the data_ingestion.py

    - export_collection_as_dataframe (mondoDB to DF)
    - export_data_into_feature_store ( save from DF to feature store so that everytime we dont take from mongoDB)
    - 



12/6: 277 --> 283 Project Setup, mongoDB setup, ETL pipeline (csv to mongoDB json) - COMPLETE
13/6: 284 --> 288 ( Data ingestion + Data validation)
14/6a: 289 --> 292 (DT, Model Trainer, model evaluation with HP) --> morning
14/6b: 293 --> 297 (Exp tracking, model pusher, MT pipeline, Batch prediction Pipeline) --> afternoon
14/6c: 298 --> 301 (Push to AWS S3, Docker image, github action to ECR, final to EC2) --> evening
15/6: CV + 3 projects github
16/6: plan for Next 80 days
17/6: Pack
18/6: Sneho + Joi
-------------------

3 mlops project
Revised till ML 
Revised AWS 
Planned for projects 
CV version 3 ready
------------------
